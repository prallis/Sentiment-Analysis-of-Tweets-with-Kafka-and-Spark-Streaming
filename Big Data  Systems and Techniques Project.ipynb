{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Create a parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pyspark dataframe reading by the prostgres database products_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- upc_id: string (nullable = true)\n",
      " |-- descr: string (nullable = true)\n",
      " |-- vendor_catalog_url: string (nullable = true)\n",
      " |-- buy_url: string (nullable = true)\n",
      " |-- manufacturer_name: string (nullable = true)\n",
      " |-- sale_price: decimal(38,18) (nullable = true)\n",
      " |-- retail_price: decimal(38,18) (nullable = true)\n",
      " |-- manufacturer_part_no: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "+----------+--------------------+------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------+---------+-------------+-------------+-----------+\n",
      "|product_id|                name|upc_id|               descr|  vendor_catalog_url|             buy_url|manufacturer_name|          sale_price|        retail_price|manufacturer_part_no|country|vendor_id|category_name|category_code|category_id|\n",
      "+----------+--------------------+------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------+---------+-------------+-------------+-----------+\n",
      "|    444554|Mou 'Eskimo 50' b...|  null|Black sheep skin ...|http://www.shopst...|http://www.shopst...|              Mou|462.7200000000000...|462.7200000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    449588|Dr. Martens '1460...|  null|Black leather '14...|http://www.shopst...|http://www.shopst...|      Dr. Martens|168.1600000000000...|168.1600000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    441781|La Canadienne Wom...|  null|La Canadienne Wom...|http://www.shopst...|http://www.shopst...|    La Canadienne|249.9500000000000...|249.9500000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442197| LE SILLA Tall boots|  null|Zip closure<br>Ro...|http://www.shopst...|http://www.shopst...|         Le Silla|1210.000000000000...|1210.000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442199|JIL SANDER Ankle ...|  null|Round toeline<br>...|http://www.shopst...|http://www.shopst...|       Jil Sander|995.0000000000000...|995.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442254|   Jenni Kayne Boots|  null|Black Jenny Kayne...|http://www.shopst...|http://www.shopst...|      Jenni Kayne|115.0000000000000...|115.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442255|    Jimmy Choo Boots|  null|Tan Jimmy Choo bo...|http://www.shopst...|http://www.shopst...|       Jimmy Choo|235.0000000000000...|235.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442256|    Jimmy Choo Boots|  null|Black Jimmy Choo ...|http://www.shopst...|http://www.shopst...|       Jimmy Choo|260.0000000000000...|260.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    442257|   Gucci Ankle Boots|  null|Black Gucci suede...|http://www.shopst...|http://www.shopst...|            Gucci|295.0000000000000...|295.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "|    444723|VIC MATIE' Over t...|  null|Zip closure<br>Ro...|http://www.shopst...|http://www.shopst...|        Vic Mati�|965.0000000000000...|965.0000000000000...|                null|   null|     null|        Boots|        boots|       1069|\n",
      "+----------+--------------------+------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------+---------+-------------+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://s01:5432/products_project\") \\\n",
    "    .option(\"driver\",\"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(select * from shoes ) as shoes\") \\\n",
    "    .option(\"user\", \"prallis_ds\") \\\n",
    "    .option(\"password\", \"13131966\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the DataFrame in HDFS in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"hdfs:///products_project/shoes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from Course 7 Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "\n",
    "class CrossValidatorVerbose(CrossValidator):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        metricName = eva.getMetricName()\n",
    "\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            foldNum = i + 1\n",
    "            print(\"Comparing models on fold %d\" % foldNum)\n",
    "\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition)\n",
    "            train = df.filter(~condition)\n",
    "\n",
    "            for j in range(numModels):\n",
    "                paramMap = epm[j]\n",
    "                model = est.fit(train, paramMap)\n",
    "                # TODO: duplicate evaluator to take extra params from input\n",
    "                metric = eva.evaluate(model.transform(validation, paramMap))\n",
    "                metrics[j] += metric\n",
    "\n",
    "                avgSoFar = metrics[j] / foldNum\n",
    "                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n",
    "                    {param.name: val for (param, val) in paramMap.items()},\n",
    "                    metricName, metric, avgSoFar))\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "\n",
    "        bestParams = epm[bestIndex]\n",
    "        bestModel = est.fit(dataset, bestParams)\n",
    "        avgMetrics = [m / nFolds for m in metrics]\n",
    "        bestAvg = avgMetrics[bestIndex]\n",
    "        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n",
    "            {param.name: val for (param, val) in bestParams.items()},\n",
    "            metricName, bestAvg))\n",
    "\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", outPath])\n",
    "shoes_parquet = spark.read.parquet(\"hdfs:///products_project/shoes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|               descr|category_code|\n",
      "+--------------------+-------------+\n",
      "|Black sheep skin ...|        boots|\n",
      "|Black leather '14...|        boots|\n",
      "|La Canadienne Wom...|        boots|\n",
      "|Zip closure<br>Ro...|        boots|\n",
      "|Round toeline<br>...|        boots|\n",
      "+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = shoes_parquet.select('descr','category_code')\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-features#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing models on fold 1\n",
      "params: {'numTrees': 10, 'numFeatures': 10}\taccuracy: 0.468217\tavg: 0.468217\n",
      "params: {'numTrees': 20, 'numFeatures': 10}\taccuracy: 0.473920\tavg: 0.473920\n",
      "params: {'numTrees': 10, 'numFeatures': 1000}\taccuracy: 0.599737\tavg: 0.599737\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.605830\tavg: 0.605830\n",
      "params: {'numTrees': 10, 'numFeatures': 10000}\taccuracy: 0.503802\tavg: 0.503802\n",
      "params: {'numTrees': 20, 'numFeatures': 10000}\taccuracy: 0.476748\tavg: 0.476748\n",
      "Comparing models on fold 2\n",
      "params: {'numTrees': 10, 'numFeatures': 10}\taccuracy: 0.469645\tavg: 0.468931\n",
      "params: {'numTrees': 20, 'numFeatures': 10}\taccuracy: 0.474323\tavg: 0.474121\n",
      "params: {'numTrees': 10, 'numFeatures': 1000}\taccuracy: 0.603927\tavg: 0.601832\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.615523\tavg: 0.610677\n",
      "params: {'numTrees': 10, 'numFeatures': 10000}\taccuracy: 0.503070\tavg: 0.503436\n",
      "params: {'numTrees': 20, 'numFeatures': 10000}\taccuracy: 0.468378\tavg: 0.472563\n",
      "Comparing models on fold 3\n",
      "params: {'numTrees': 10, 'numFeatures': 10}\taccuracy: 0.470321\tavg: 0.469394\n",
      "params: {'numTrees': 20, 'numFeatures': 10}\taccuracy: 0.478529\tavg: 0.475591\n",
      "params: {'numTrees': 10, 'numFeatures': 1000}\taccuracy: 0.634032\tavg: 0.612565\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.630710\tavg: 0.617354\n",
      "params: {'numTrees': 10, 'numFeatures': 10000}\taccuracy: 0.574674\tavg: 0.527182\n",
      "params: {'numTrees': 20, 'numFeatures': 10000}\taccuracy: 0.505105\tavg: 0.483410\n",
      "Comparing models on fold 4\n",
      "params: {'numTrees': 10, 'numFeatures': 10}\taccuracy: 0.465941\tavg: 0.468531\n",
      "params: {'numTrees': 20, 'numFeatures': 10}\taccuracy: 0.471901\tavg: 0.474668\n",
      "params: {'numTrees': 10, 'numFeatures': 1000}\taccuracy: 0.589814\tavg: 0.606878\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.622273\tavg: 0.618584\n",
      "params: {'numTrees': 10, 'numFeatures': 10000}\taccuracy: 0.453726\tavg: 0.508818\n",
      "params: {'numTrees': 20, 'numFeatures': 10000}\taccuracy: 0.487810\tavg: 0.484510\n",
      "Comparing models on fold 5\n",
      "params: {'numTrees': 10, 'numFeatures': 10}\taccuracy: 0.469128\tavg: 0.468650\n",
      "params: {'numTrees': 20, 'numFeatures': 10}\taccuracy: 0.474269\tavg: 0.474588\n",
      "params: {'numTrees': 10, 'numFeatures': 1000}\taccuracy: 0.599912\tavg: 0.605484\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.628899\tavg: 0.620647\n",
      "params: {'numTrees': 10, 'numFeatures': 10000}\taccuracy: 0.464672\tavg: 0.499989\n",
      "params: {'numTrees': 20, 'numFeatures': 10000}\taccuracy: 0.502326\tavg: 0.488073\n",
      "Best model:\n",
      "params: {'numTrees': 20, 'numFeatures': 1000}\taccuracy: 0.620647\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"category_code\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"descr\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol='features', numTrees=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, tokenizer, hashingTF, idf,  rf, labelConverter])\n",
    "\n",
    "(train, test) = data.randomSplit([0.7, 0.3])\n",
    "train.cache()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 1000, 10000]) \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidatorVerbose(estimator=pipeline,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                      numFolds=5) \n",
    "\n",
    "rf_model = crossval.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.414736 \n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[descr: string, category_code: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"category_code\", outputCol=\"label\").fit(data)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"descr\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "(train, test) = data.randomSplit([0.7, 0.3])\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, tokenizer, hashingTF,idf , ovr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 1000, 10000]) \\\n",
    "    .build()\n",
    "crossval = CrossValidatorVerbose(estimator=pipeline,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                      numFolds=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing models on fold 1\n",
      "params: {'numFeatures': 10}\taccuracy: 0.463998\tavg: 0.463998\n",
      "params: {'numFeatures': 1000}\taccuracy: 0.897535\tavg: 0.897535\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.946156\tavg: 0.946156\n",
      "Comparing models on fold 2\n",
      "params: {'numFeatures': 10}\taccuracy: 0.457295\tavg: 0.460646\n",
      "params: {'numFeatures': 1000}\taccuracy: 0.894005\tavg: 0.895770\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.945710\tavg: 0.945933\n",
      "Comparing models on fold 3\n",
      "params: {'numFeatures': 10}\taccuracy: 0.462781\tavg: 0.461358\n",
      "params: {'numFeatures': 1000}\taccuracy: 0.896687\tavg: 0.896076\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.948515\tavg: 0.946793\n",
      "Comparing models on fold 4\n",
      "params: {'numFeatures': 10}\taccuracy: 0.463759\tavg: 0.461958\n",
      "params: {'numFeatures': 1000}\taccuracy: 0.898209\tavg: 0.896609\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.947994\tavg: 0.947094\n",
      "Comparing models on fold 5\n",
      "params: {'numFeatures': 10}\taccuracy: 0.461410\tavg: 0.461849\n",
      "params: {'numFeatures': 1000}\taccuracy: 0.896718\tavg: 0.896631\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.945783\tavg: 0.946831\n",
      "Best model:\n",
      "params: {'numFeatures': 10000}\taccuracy: 0.946831\n"
     ]
    }
   ],
   "source": [
    "# train the multiclass model.\n",
    "ovrModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0511128\n"
     ]
    }
   ],
   "source": [
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = ovrModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = \"hdfs:///products_project/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'overwrite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5737eb079948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'overwrite'"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "lr_model.save(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/39776617/spark-streaming-how-to-load-a-pipeline-on-a-stream|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "outPath = \"hdfs:///products_project/model\"\n",
    "load_lr_model = PipelineModel.load(outPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|               descr|category_code|label|               words|         rawFeatures|            features|prediction|\n",
      "+--------------------+-------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|\"A blast\" is what...|      sandals|  1.0|[\"a, blast\", is, ...|(10000,[167,628,7...|(10000,[167,628,7...|       1.0|\n",
      "|\"A “multi-talente...|      sandals|  1.0|[\"a, “multi-talen...|(10000,[638,848,9...|(10000,[638,848,9...|       1.0|\n",
      "|\"Can a casual sli...|      sandals|  1.0|[\"can, a, casual,...|(10000,[100,304,5...|(10000,[100,304,5...|       1.0|\n",
      "|\"Casual\" just sla...|mens-sneakers|  3.0|[\"casual\", just, ...|(10000,[161,307,4...|(10000,[161,307,4...|       3.0|\n",
      "|\"Modern and sexy,...|      sandals|  1.0|[\"modern, and, se...|(10000,[36,158,28...|(10000,[36,158,28...|       1.0|\n",
      "|\"Mom they make me...|  girls-shoes|  2.0|[\"mom, they, make...|(10000,[264,763,1...|(10000,[264,763,1...|       2.0|\n",
      "|\"Named after my b...|      sandals|  1.0|[\"named, after, m...|(10000,[158,523,7...|(10000,[158,523,7...|       1.0|\n",
      "|\"Named for Serge ...|        boots|  0.0|[\"named, for, ser...|(10000,[158,230,2...|(10000,[158,230,2...|       0.0|\n",
      "|\"Our designer gav...|      sandals|  1.0|[\"our, designer, ...|(10000,[524,572,9...|(10000,[524,572,9...|       1.0|\n",
      "|\"Peri\" is one of ...|        boots|  0.0|[\"peri\", is, one,...|(10000,[91,128,52...|(10000,[91,128,52...|       0.0|\n",
      "+--------------------+-------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:36\n",
      "-------------------------------------------\n",
      "here you can find sneakers offers #shoesoffers\n",
      "the best shoes offers for ladies #shoesoffers\n",
      "boots offers in summer why not\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-04 12:32:43\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8065e568d0c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/streaming/context.pyc\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils, TopicAndPartition\n",
    "from pyspark.streaming import StreamingContext\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "topicPartion = TopicAndPartition('offers',0)\n",
    "topic = 'offers'\n",
    "fromOffset = {topicPartion: 0}\n",
    "\n",
    "twitterKafkaStream = KafkaUtils.createDirectStream(ssc, [topic],{\"bootstrap.servers\": 'localhost:9092'}, fromOffsets=fromOffset)\n",
    "\n",
    "tweets = twitterKafkaStream. \\\n",
    "        map(lambda (key, value): json.loads(value)). \\\n",
    "        map(lambda json_object: (json_object[\"text\"]))\n",
    "\n",
    "tweets.saveAsTextFiles('/tweets/')\n",
    "tweets.pprint(10)\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets = spark.read.csv(\"hdfs:///tweets/-1593865956000/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|here you can find...|\n",
      "|the best shoes of...|\n",
      "|boots offers in s...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tweets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- descr: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               descr|\n",
      "+--------------------+\n",
      "|here you can find...|\n",
      "|the best shoes of...|\n",
      "|boots offers in s...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oldColumns = new_tweets.schema.names\n",
    "newColumns = [\"descr\"]\n",
    "\n",
    "df = reduce(lambda new_tweets, idx: new_tweets.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), new_tweets)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "outPath = \"hdfs:///products_project/model\"\n",
    "load_lr_model = PipelineModel.load(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_lr_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|               descr|               words|         rawFeatures|            features|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|here you can find...|[here, you, can, ...|(10000,[1135,1425...|(10000,[1135,1425...|       3.0|\n",
      "|the best shoes of...|[the, best, shoes...|(10000,[219,763,1...|(10000,[219,763,1...|       1.0|\n",
      "|boots offers in s...|[boots, offers, i...|(10000,[1445,3965...|(10000,[1445,3965...|       0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"category_code\", outputCol=\"label\").fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = labelIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----+\n",
      "|               descr|category_code|label|\n",
      "+--------------------+-------------+-----+\n",
      "|Orange 'Flyknit' ...|mens-sneakers|  3.0|\n",
      "+--------------------+-------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.filter(label_df['label'] == 3 ).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write.parquet(\"hdfs:///tweets/predictions/predictions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            combined|\n",
      "+--------------------+\n",
      "|here you can find...|\n",
      "|the best shoes of...|\n",
      "|boots offers in s...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "def myConcat(*cols):\n",
    "    concat_columns = []\n",
    "    for c in cols[:-1]:\n",
    "        concat_columns.append(F.coalesce(c, F.lit(\"*\")))\n",
    "        concat_columns.append(F.lit(\" \"))  \n",
    "    concat_columns.append(F.coalesce(cols[-1], F.lit(\"*\")))\n",
    "    return F.concat(*concat_columns)\n",
    "\n",
    "df_text = predictions.withColumn(\"combined\", myConcat(*df.columns)).select(\"combined\")\n",
    "\n",
    "df_text.show()\n",
    "\n",
    "df_text.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(\"hdfs:///tweets/predictions/predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
